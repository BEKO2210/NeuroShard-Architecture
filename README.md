# NeuroShard â€“ Experimental Mixture-of-Experts Architecture

**Author:** Belkis Aslani (`BEKO2210`)  
**Repository:** https://github.com/BEKO2210/NeuroShard-Architecture  

NeuroShard is an experimental, lightweight **Mixture-of-Experts (MoE)** architecture designed to test whether:
- simple learned routers  
- low-rank expert matrices  
- and compact embeddings  
can already create meaningful topic separation and directional feature behavior without large models.

This project runs on:
- **Windows + VS Code + Python 3.12 + PyTorch 2.9.1**
- **Android (Termux) in a pure-Python, no-torch fallback version**

---

## ğŸš€ Key Features

- **Versioned Architecture**
  - **v1:** Fixed keyword-based router  
  - **v2:** Fully *learned* router (Cross-Entropy + MSE training)  
  - **v3:** Larger dataset, better topic separation, more stable outputs  

- **Low-Rank Shards**
  Each â€œexpertâ€ is a learned low-rank adapter:  
  \[
  S_i = U_i \cdot V_i^T
  \]
  Efficient, fast, extremely small in parameter count.

- **Learned Router**
  A small neural network that maps embeddings â†’ softmax topic distribution.

- **Modular Implementation**
  All code is separated into:
  - `src/` (training + testing code)
  - `models/` (saved checkpoints)
  - `data/` (datasets)
  - `experiments/` (log files)
  - `whitepaper/` (LaTeX scientific documentation)

---

## ğŸ“ Project Structure

```text
NeuroShard-Architecture/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_neuroshard.py
â”‚   â”œâ”€â”€ test_neuroshard.py
â”‚   â”œâ”€â”€ train_neuroshard_v2_router.py
â”‚   â”œâ”€â”€ test_neuroshard_v2_router.py
â”‚   â”œâ”€â”€ train_neuroshard_v3_bigdata.py
â”‚   â”œâ”€â”€ test_neuroshard_v3_bigdata.py
â”‚   â”œâ”€â”€ neuroshard_repl.py
â”‚   â”œâ”€â”€ neuroshard_multilayer.py
â”‚   â””â”€â”€ neuroshard_repl_topics.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_v1_small.txt
â”‚   â”œâ”€â”€ dataset_v3_big.txt
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ neuroshard_v1.pth
â”‚   â”œâ”€â”€ neuroshard_v2_router.pth
â”‚   â”œâ”€â”€ neuroshard_v3_bigdata.pth
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ logs_v1.txt
â”‚   â”œâ”€â”€ logs_v2.txt
â”‚   â”œâ”€â”€ logs_v3.txt
â”‚
â”œâ”€â”€ whitepaper/
â”‚   â”œâ”€â”€ NeuroShard_Whitepaper.tex
â”‚   â””â”€â”€ figures/
â”‚        â””â”€â”€ architecture_diagram.png
â”‚
â””â”€â”€ README.md
ğŸ§  Architecture Overview
1. Embedding Layer
A compact text embedding:

letter-frequency vector

no external models

works offline

extremely fast (mobile-friendly)

2. Base Transformation
A linear projection:

â„
=
ğ‘Š
base
â‹…
ğ‘¥
h=W 
base
â€‹
 â‹…x
3. Shards (Experts)
Each shard is low-rank:

ğ‘†
ğ‘–
=
ğ‘ˆ
ğ‘–
â‹…
ğ‘‰
ğ‘–
ğ‘‡
S 
i
â€‹
 =U 
i
â€‹
 â‹…V 
i
T
â€‹
 
They add topic-specific direction:

ğ‘œ
ğ‘–
=
ğ‘†
ğ‘–
â‹…
ğ‘¥
o 
i
â€‹
 =S 
i
â€‹
 â‹…x
4. Router
A small MLP or linear layer producing:

ğ›¼
=
softmax
(
ğ‘…
(
ğ‘¥
)
)
Î±=softmax(R(x))
5. Output Fusion
output
=
â„
+
âˆ‘
ğ‘–
ğ›¼
ğ‘–
â‹…
ğ‘œ
ğ‘–
output=h+ 
i
âˆ‘
â€‹
 Î± 
i
â€‹
 â‹…o 
i
â€‹
 
ğŸ§ª Experimental Results (Summary)
v2 â€“ Learned Router
Strong separation:

Input	Router Î± (short)	Dominant
"street gang punchline rap"	[0.9998, ...]	Rap
"pure love everyone peace"	[0.00005, 0.9998, ...]	Soft
"advanced integral theorem math"	[0.00008, 0.00014, 0.9996, ...]	Math
"vogel hund katze bÃ¤r"	[0.00005, 0.00008, 0.00008, 0.9997]	Animals

Outputs show consistent vector direction changes per topic.

v3 â€“ More Data
Better generalization, smoother routing, stronger cross-topic mixing.

ğŸ›  Installation
Clone
bash
Code kopieren
git clone https://github.com/BEKO2210/NeuroShard-Architecture.git
cd NeuroShard-Architecture
Virtual Environment (Windows)
bash
Code kopieren
python -m venv .venv
.\.venv\Scripts\activate
Install Dependencies
bash
Code kopieren
pip install torch numpy
â–¶ï¸ Usage
Train v1
bash
Code kopieren
cd src
python train_neuroshard.py
Test v1
bash
Code kopieren
python test_neuroshard.py
Train v2 (learned router)
bash
Code kopieren
python train_neuroshard_v2_router.py
Train v3 (big dataset)
bash
Code kopieren
python train_neuroshard_v3_bigdata.py
ğŸ“„ Whitepaper
The scientific LaTeX whitepaper can be found here:

Code kopieren
whitepaper/NeuroShard_Whitepaper.tex
It includes:

full mathematical formulation

diagrams

experiments

limitations

future improvements

ğŸ”® Future Work
better embeddings (subword, n-gram, hashed embeddings)

more experts (8â€“32 shards)

multi-layer NeuroShard blocks

GPU-optimized variant

integration into LLM preprocessing

ğŸ“œ License
Distributed under the Apache-2.0 License.

â­ Acknowledgements
This is an independent research experiment created by Belkis Aslani.
The goal is to explore extremely lightweight neural architectures that can run everywhere â€” even on mobile.