\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{fancyhdr}

% Page Layout
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\lhead{NeuroShard Architecture}
\rhead{B. Aslani (2025)}
\cfoot{\thepage}

% Title Info
\title{\textbf{NeuroShard: An Experimental Lightweight Mixture-of-Experts Architecture}}
\author{\textbf{Belkis Aslani} \\
\textit{Independent Researcher} \\
\texttt{github.com/BEKO2210/NeuroShard-Architecture}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents \textbf{NeuroShard}, an experimental neural architecture designed to explore the efficiency of Mixture-of-Experts (MoE) mechanisms in highly constrained environments. Unlike modern Large Language Models (LLMs) which require massive computational resources, NeuroShard utilizes a compact, letter-frequency-based embedding system coupled with low-rank expert matrices. We demonstrate that a small, learned router can effectively separate distinct semantic topics (e.g., Rap, Mathematics, Emotional prose) and activate specialized "shards" (experts) to transform input vectors directionally. The results suggest that semantic separation is possible with minimal parameter counts, offering potential pathways for mobile-first and offline AI utilities.
\end{abstract}

\tableofcontents
\vspace{1cm}
\hrule
\vspace{1cm}

\section{Introduction}
The current landscape of Artificial Intelligence is dominated by Transformer-based architectures scaling into billions of parameters. While effective, these models are often impractical for local execution on mobile devices or in offline scenarios.

\textbf{NeuroShard} posits a different question: \textit{Can we achieve meaningful semantic routing and feature transformation using simple linear algebra and learned gating mechanisms?}

This project implements a modular MoE architecture from scratch, using PyTorch for training and a pure-Python fallback for inference on Android (Termux). The core innovation lies in the combination of \textbf{Low-Rank Shards} and a \textbf{Learned Semantic Router}.

\section{Mathematical Formulation}

\subsection{Input Representation (Embeddings)}
To maintain extreme lightweight characteristics, NeuroShard avoids heavy pre-trained embeddings (like BERT or Word2Vec). Instead, it uses a deterministic letter-frequency mapping or simple hashing:
\begin{equation}
    x \in \mathbb{R}^{d_{in}}
\end{equation}
Where $x$ represents the normalized frequency vector of the input string.

\subsection{The Base Transformation}
Every input undergoes a shared fundamental transformation, representing the "general knowledge" or common features of the data:
\begin{equation}
    h = W_{base} \cdot x + b_{base}
\end{equation}

\subsection{Low-Rank Experts (Shards)}
Standard MoE models often use full feed-forward networks as experts. NeuroShard utilizes \textbf{Low-Rank Adapters} to minimize memory usage. Each expert $i$ is defined as:
\begin{equation}
    S_i = U_i \cdot V_i^T
\end{equation}
Where $U_i \in \mathbb{R}^{d_{out} \times r}$ and $V_i \in \mathbb{R}^{d_{in} \times r}$, with rank $r \ll d_{in}$.
The output of a specific expert is:
\begin{equation}
    o_i = S_i \cdot x
\end{equation}

\subsection{The Router (Gating Mechanism)}
The routing network determines which expert is best suited for the input. It is a learned Multi-Layer Perceptron (MLP):
\begin{equation}
    \text{logits} = R(x)
\end{equation}
\begin{equation}
    \alpha = \text{softmax}(\text{logits})
\end{equation}
Here, $\alpha_i$ represents the confidence that the input belongs to topic $i$.

\subsection{Output Fusion}
The final output is a weighted summation of the base signal and the expert opinions:
\begin{equation}
    y = h + \sum_{i=1}^{N} \alpha_i \cdot o_i
\end{equation}

\section{Architecture Evolution}

\subsection{v1: Static Routing}
The initial prototype used keyword matching to force routing weights $\alpha$. While functional, it did not generalize to unseen words.

\subsection{v2: Learned Routing}
Introduced a trainable router optimized via Cross-Entropy Loss against labeled topic data, simultaneous with the reconstruction loss (MSE) for the experts.
\begin{equation}
    L_{total} = \lambda_1 L_{MSE}(y, y_{target}) + \lambda_2 L_{CrossEntropy}(\alpha, \text{label})
\end{equation}

\subsection{v3: Big Data \& Generalization}
Scaled the dataset to include diverse corpus entries. The model demonstrated the ability to route ambiguous inputs (e.g., "love and logic") by activating multiple experts simultaneously (soft routing).

\section{Experimental Results}

Experiments were conducted on a local Windows environment (PyTorch) and verified on Android.

\begin{table}[h]
    \centering
    \begin{tabular}{@{}lp{4cm}l@{}}
    \toprule
    \textbf{Input String} & \textbf{Router Activation ($\alpha$)} & \textbf{Dominant Topic} \\ \midrule
    "street gang rap" & $[0.99, 0.00, 0.00, 0.00]$ & Rap / Urban \\
    "pure love peace" & $[0.00, 0.99, 0.00, 0.00]$ & Soft / Soul \\
    "integral theorem" & $[0.00, 0.00, 0.99, 0.00]$ & Math / Logic \\
    "hund katze b√§r" & $[0.00, 0.00, 0.00, 0.99]$ & Animals (DE) \\ \bottomrule
    \end{tabular}
    \caption{Routing behavior in NeuroShard v2. The model successfully identifies semantics based on character distribution alone.}
    \label{tab:results}
\end{table}

\section{Limitations}
\begin{itemize}
    \item \textbf{Embeddings:} Letter-frequency embeddings lose word order information ("dog bites man" = "man bites dog").
    \item \textbf{Capacity:} The low-rank constraint limits the complexity of features each shard can learn.
\end{itemize}

\section{Conclusion \& Future Work}
NeuroShard demonstrates that MoE architectures do not strictly require massive scale to function. The separation of concerns (Router vs. Experts) works even with minimal parameters. Future work will focus on integrating \textit{subword tokenizers} and exploring \textit{Multi-Head Routing} to capture nuances in longer texts.

\end{document}
